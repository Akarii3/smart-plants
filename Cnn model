# import necessary libraries
import tensorflow as tf
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np

# Step 1: Define Parameters
IMAGESIZE = (224, 224)  # Input image size
BATCH_SIZE = 32  # Batch size for training
NUM_EPOCHS = 20  # Number of training epochs (increase if necessary)
NUM_CLASSES = 13  # Number of plant classes
TRAIN_DATA_DIR = '/home/akari/Downloads/Dataset/Plant Dataset'  # Path to training data

# Step 2: Load and preprocess data using ImageDataGenerator with strong augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,  # Normalize pixel values to [0, 1]
    rotation_range=40,  # Rotate images randomly between -40 and 40 degrees
    width_shift_range=0.3,  # Shift images horizontally by 30% of the total width
    height_shift_range=0.3,  # Shift images vertically by 30% of the total height
    shear_range=0.3,  # Apply shear transformation
    zoom_range=0.3,  # Zoom images randomly by up to 30%
    horizontal_flip=True,  # Flip images horizontally with a probability of 50%
    vertical_flip=True,  # Flip images vertically with a probability of 50%
    brightness_range=[0.5, 1.5],  # Randomly change brightness between 50% and 150%
    channel_shift_range=50.0,  # Randomly shift color channels (Red, Green, Blue)
    fill_mode='nearest',  # Fill any empty pixels after transformation
    validation_split=0.2  # Use 20% of the data for validation
)

train_generator = train_datagen.flow_from_directory(
    TRAIN_DATA_DIR,
    target_size=IMAGESIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'  # Use the training portion of the data
)

# Create a validation generator from the same training data using the validation_split
validation_generator = train_datagen.flow_from_directory(
    TRAIN_DATA_DIR,
    target_size=IMAGESIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'  # Use the validation portion of the data (20%)
)

# Step 3: Build the CNN model using transfer learning
# Load pre-trained MobileNet model (excluding the top layers)
base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Add custom layers on top of the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)  # Add Dropout layer with 50% probability to reduce overfitting
predictions = Dense(NUM_CLASSES, activation='softmax')(x)

# Create the final model
model = Model(inputs=base_model.input, outputs=predictions)

# Freeze the base model layers to use it for feature extraction (fine-tuning top layers)
for layer in base_model.layers:
    layer.trainable = False  # Freeze the base layers initially

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Step 4: Train the model with early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    train_generator,
    epochs=NUM_EPOCHS,
    validation_data=validation_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    validation_steps=validation_generator.samples // BATCH_SIZE,
    callbacks=[early_stopping]
)

# Step 5: Save the trained model
model.save('plant_classification_cnn.h5')

# Step 6: Convert the model to TensorFlow Lite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open('plant_classification_cnn.tflite', 'wb') as f:
    f.write(tflite_model)

print("Model saved and converted to TensorFlow Lite format.")
